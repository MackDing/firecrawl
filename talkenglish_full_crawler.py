import requests
import json
import time
import os
import re
import urllib.parse
from datetime import datetime
from pathlib import Path
import mimetypes
from urllib.parse import urljoin, urlparse

class TalkEnglishFullCrawler:
    def __init__(self, base_url="http://localhost:3002", config_file="talkenglish_full_media_config.json"):
        self.base_url = base_url
        self.config_file = config_file
        
        # åˆ›å»ºå¸¦æ—¥æœŸçš„ç›®å½•å
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.session_dir = Path("results") / f"talkenglish_{self.timestamp}"
        
        # å­ç›®å½•ç»“æ„
        self.audio_dir = self.session_dir / "audio"
        self.video_dir = self.session_dir / "video"
        self.images_dir = self.session_dir / "images"
        self.content_dir = self.session_dir / "content"
        self.reports_dir = self.session_dir / "reports"
        self.raw_data_dir = self.session_dir / "raw_data"
        
        # åˆ›å»ºç›®å½•ç»“æ„
        self.create_directories()
        
        self.headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        }
        
        # åª’ä½“æ–‡ä»¶ç»Ÿè®¡
        self.media_stats = {
            'audio': {'found': 0, 'downloaded': 0, 'failed': 0},
            'video': {'found': 0, 'downloaded': 0, 'failed': 0},
            'images': {'found': 0, 'downloaded': 0, 'failed': 0}
        }
    
    def create_directories(self):
        """åˆ›å»ºå®Œæ•´çš„ç›®å½•ç»“æ„"""
        directories = [
            self.session_dir,
            self.audio_dir,
            self.video_dir,
            self.images_dir,
            self.content_dir,
            self.reports_dir,
            self.raw_data_dir,
            # å†…å®¹åˆ†ç±»ç›®å½•
            self.content_dir / "lessons",
            self.content_dir / "speaking",
            self.content_dir / "listening",
            self.content_dir / "grammar",
            self.content_dir / "vocabulary",
            self.content_dir / "pronunciation",
            self.content_dir / "beginner",
            self.content_dir / "intermediate",
            self.content_dir / "advanced",
            self.content_dir / "business",
            self.content_dir / "travel",
            self.content_dir / "toefl",
            self.content_dir / "ielts",
            self.content_dir / "interview",
            self.content_dir / "practice",
            self.content_dir / "other"
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“ å·²åˆ›å»ºä¼šè¯ç›®å½•: {self.session_dir.absolute()}")
        print(f"ğŸ“… ä¼šè¯æ—¶é—´: {self.timestamp}")
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print(f"âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶: {self.config_file}")
            return config
        except FileNotFoundError:
            print(f"âŒ é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {self.config_file}")
            return None
        except json.JSONDecodeError as e:
            print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            return None
    
    def start_crawl(self):
        """å¯åŠ¨å…¨åª’ä½“çˆ¬å–"""
        config = self.load_config()
        if not config:
            return None
        
        print("ğŸš€ å¯åŠ¨ TalkEnglish.com å…¨åª’ä½“çˆ¬å–...")
        print(f"ğŸ¯ ç›®æ ‡ç½‘ç«™: {config.get('url')}")
        print(f"ğŸ“Š é…ç½®è¯¦æƒ…:")
        print(f"  - é¡µé¢é™åˆ¶: {config.get('limit', 'N/A')}")
        print(f"  - æœ€å¤§æ·±åº¦: {config.get('maxDepth', 'N/A')}")
        print(f"  - ç­‰å¾…æ—¶é—´: {config.get('scrapeOptions', {}).get('waitFor', 'N/A')}ms")
        print(f"  - è¶…æ—¶æ—¶é—´: {config.get('scrapeOptions', {}).get('timeout', 'N/A')}ms")
        print(f"  - åŒ…å«è·¯å¾„: {len(config.get('includePaths', []))} ä¸ª")
        print(f"  - æ’é™¤è·¯å¾„: {len(config.get('excludePaths', []))} ä¸ª")
        print(f"ğŸ“ ç»“æœä¿å­˜åˆ°: {self.session_dir.absolute()}")
        
        response = requests.post(
            f"{self.base_url}/v1/crawl",
            headers=self.headers,
            json=config
        )
        
        if response.status_code == 200:
            result = response.json()
            crawl_id = result.get('id')
            print(f"âœ… çˆ¬å–ä»»åŠ¡å·²å¯åŠ¨ï¼ŒID: {crawl_id}")
            
            # ä¿å­˜åˆå§‹å“åº”
            initial_file = self.reports_dir / f"crawl_start.json"
            with open(initial_file, "w", encoding="utf-8") as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            return crawl_id
        else:
            print(f"âŒ å¯åŠ¨çˆ¬å–å¤±è´¥: {response.status_code}")
            print(response.text)
            return None
    
    def check_status(self, crawl_id):
        """æ£€æŸ¥çˆ¬å–çŠ¶æ€"""
        response = requests.get(
            f"{self.base_url}/v1/crawl/{crawl_id}",
            headers=self.headers
        )
        
        if response.status_code == 200:
            return response.json()
        else:
            print(f"âŒ æ£€æŸ¥çŠ¶æ€å¤±è´¥: {response.status_code}")
            return None
    
    def extract_media_urls(self, content, base_url="https://www.talkenglish.com"):
        """ä»å†…å®¹ä¸­æå–æ‰€æœ‰åª’ä½“æ–‡ä»¶URL"""
        media_urls = {
            'audio': set(),
            'video': set(),
            'images': set()
        }
        
        # éŸ³é¢‘æ–‡ä»¶æ¨¡å¼
        audio_patterns = [
            r'https?://[^\s\'")]+\.(?:mp3|wav|ogg|m4a|aac|flac)(?:\?[^\s\'")]*)?',
            r'src=["\']([^"\']*\.(?:mp3|wav|ogg|m4a|aac|flac)(?:\?[^"\']*)?)["\']',
            r'href=["\']([^"\']*\.(?:mp3|wav|ogg|m4a|aac|flac)(?:\?[^"\']*)?)["\']',
            r'data-src=["\']([^"\']*\.(?:mp3|wav|ogg|m4a|aac|flac)(?:\?[^"\']*)?)["\']',
            r'data-audio=["\']([^"\']*)["\']',
            r'audio-url=["\']([^"\']*)["\']'
        ]
        
        # è§†é¢‘æ–‡ä»¶æ¨¡å¼
        video_patterns = [
            r'https?://[^\s\'")]+\.(?:mp4|avi|mov|wmv|flv|webm|mkv|m4v)(?:\?[^\s\'")]*)?',
            r'src=["\']([^"\']*\.(?:mp4|avi|mov|wmv|flv|webm|mkv|m4v)(?:\?[^"\']*)?)["\']',
            r'data-src=["\']([^"\']*\.(?:mp4|avi|mov|wmv|flv|webm|mkv|m4v)(?:\?[^"\']*)?)["\']',
            r'data-video=["\']([^"\']*)["\']',
            r'video-url=["\']([^"\']*)["\']',
            # YouTube, Vimeoç­‰åµŒå…¥è§†é¢‘
            r'https?://(?:www\.)?youtube\.com/embed/([^"\'?\s]+)',
            r'https?://(?:www\.)?youtube\.com/watch\?v=([^"\'&\s]+)',
            r'https?://(?:www\.)?vimeo\.com/(\d+)',
            r'https?://player\.vimeo\.com/video/(\d+)'
        ]
        
        # å›¾ç‰‡æ–‡ä»¶æ¨¡å¼
        image_patterns = [
            r'https?://[^\s\'")]+\.(?:jpg|jpeg|png|gif|bmp|svg|webp|ico)(?:\?[^\s\'")]*)?',
            r'src=["\']([^"\']*\.(?:jpg|jpeg|png|gif|bmp|svg|webp|ico)(?:\?[^"\']*)?)["\']',
            r'data-src=["\']([^"\']*\.(?:jpg|jpeg|png|gif|bmp|svg|webp|ico)(?:\?[^"\']*)?)["\']',
            r'background-image:\s*url\(["\']?([^"\')\s]*\.(?:jpg|jpeg|png|gif|bmp|svg|webp))(?:\?[^"\')\s]*)?["\']?\)'
        ]
        
        # æå–éŸ³é¢‘URL
        for pattern in audio_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0] if match[0] else (match[1] if len(match) > 1 else '')
                if match:
                    # å¤„ç†ç›¸å¯¹URL
                    if match.startswith('/'):
                        match = urljoin(base_url, match)
                    elif not match.startswith('http'):
                        match = urljoin(base_url, match)
                    media_urls['audio'].add(match)
        
        # æå–è§†é¢‘URL
        for pattern in video_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0] if match[0] else (match[1] if len(match) > 1 else '')
                if match:
                    # å¤„ç†YouTube/Vimeo ID
                    if 'youtube.com' in pattern or 'vimeo.com' in pattern:
                        if 'youtube' in pattern:
                            match = f"https://www.youtube.com/watch?v={match}"
                        elif 'vimeo' in pattern:
                            match = f"https://vimeo.com/{match}"
                    elif match.startswith('/'):
                        match = urljoin(base_url, match)
                    elif not match.startswith('http'):
                        match = urljoin(base_url, match)
                    media_urls['video'].add(match)
        
        # æå–å›¾ç‰‡URL
        for pattern in image_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0] if match[0] else (match[1] if len(match) > 1 else '')
                if match:
                    if match.startswith('/'):
                        match = urljoin(base_url, match)
                    elif not match.startswith('http'):
                        match = urljoin(base_url, match)
                    media_urls['images'].add(match)
        
        # è½¬æ¢ä¸ºåˆ—è¡¨
        for media_type in media_urls:
            media_urls[media_type] = list(media_urls[media_type])
        
        return media_urls
    
    def download_media_file(self, url, media_type, filename=None):
        """ä¸‹è½½åª’ä½“æ–‡ä»¶"""
        try:
            # ç¡®å®šä¿å­˜ç›®å½•
            if media_type == 'audio':
                save_dir = self.audio_dir
            elif media_type == 'video':
                save_dir = self.video_dir
            elif media_type == 'images':
                save_dir = self.images_dir
            else:
                return None
            
            # ç”Ÿæˆæ–‡ä»¶å
            if not filename:
                parsed_url = urlparse(url)
                filename = os.path.basename(parsed_url.path)
                if not filename or '.' not in filename:
                    # æ ¹æ®åª’ä½“ç±»å‹ç”Ÿæˆé»˜è®¤æ–‡ä»¶å
                    ext_map = {
                        'audio': '.mp3',
                        'video': '.mp4',
                        'images': '.jpg'
                    }
                    timestamp = int(time.time())
                    filename = f"{media_type}_{timestamp}{ext_map.get(media_type, '')}"
            
            # ç¡®ä¿æ–‡ä»¶åå”¯ä¸€
            counter = 1
            original_filename = filename
            while (save_dir / filename).exists():
                name, ext = os.path.splitext(original_filename)
                filename = f"{name}_{counter}{ext}"
                counter += 1
            
            # ä¸‹è½½æ–‡ä»¶
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': '*/*',
                'Accept-Language': 'en-US,en;q=0.9',
                'Referer': 'https://www.talkenglish.com/'
            }
            
            response = requests.get(url, stream=True, headers=headers, timeout=60)
            response.raise_for_status()
            
            file_path = save_dir / filename
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            return {
                'filename': filename,
                'local_path': str(file_path),
                'file_size': file_path.stat().st_size,
                'url': url
            }
            
        except Exception as e:
            print(f"âŒ ä¸‹è½½{media_type}æ–‡ä»¶å¤±è´¥ {url}: {e}")
            return None
    
    def monitor_crawl(self, crawl_id, check_interval=90):
        """ç›‘æ§çˆ¬å–è¿›åº¦"""
        print(f"ğŸ“Š å¼€å§‹ç›‘æ§çˆ¬å–è¿›åº¦ (æ¯{check_interval}ç§’æ£€æŸ¥ä¸€æ¬¡)...")
        print("âš ï¸  å…¨åª’ä½“çˆ¬å–å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        start_time = datetime.now()
        last_page_count = 0
        
        while True:
            status = self.check_status(crawl_id)
            if not status:
                break
            
            current_time = datetime.now().strftime("%H:%M:%S")
            elapsed = datetime.now() - start_time
            
            if status.get('status') == 'completed':
                print(f"ğŸ‰ [{current_time}] çˆ¬å–å®Œæˆ! æ€»è€—æ—¶: {elapsed}")
                print(f"ğŸ“„ æ€»é¡µé¢æ•°: {len(status.get('data', []))}")
                
                # ä¿å­˜å®Œæ•´ç»“æœå¹¶æå–åª’ä½“
                self.save_results_and_extract_media(crawl_id, status)
                break
            
            elif status.get('status') == 'scraping':
                completed = len(status.get('data', []))
                new_pages = completed - last_page_count
                last_page_count = completed
                
                # å®æ—¶æå–åª’ä½“URLç»Ÿè®¡
                if new_pages > 0:
                    recent_pages = status.get('data', [])[-new_pages:] if new_pages > 0 else []
                    self.update_media_stats(recent_pages)
                
                print(f"â³ [{current_time}] çˆ¬å–ä¸­... å·²å®Œæˆ: {completed} é¡µé¢ (+{new_pages}) | "
                      f"éŸ³é¢‘: {self.media_stats['audio']['found']} | "
                      f"è§†é¢‘: {self.media_stats['video']['found']} | "
                      f"å›¾ç‰‡: {self.media_stats['images']['found']} | "
                      f"è€—æ—¶: {elapsed}")
            
            elif status.get('status') == 'failed':
                print(f"âŒ [{current_time}] çˆ¬å–å¤±è´¥ (è€—æ—¶: {elapsed})")
                print(f"é”™è¯¯ä¿¡æ¯: {status.get('error', 'Unknown error')}")
                break
            
            time.sleep(check_interval)
    
    def update_media_stats(self, pages):
        """æ›´æ–°åª’ä½“ç»Ÿè®¡"""
        for page in pages:
            content = page.get('markdown', '') + page.get('html', '')
            media_urls = self.extract_media_urls(content)
            
            for media_type, urls in media_urls.items():
                self.media_stats[media_type]['found'] += len(urls)
    
    def save_results_and_extract_media(self, crawl_id, status):
        """ä¿å­˜ç»“æœå¹¶æå–æ‰€æœ‰åª’ä½“æ–‡ä»¶"""
        print("\n" + "="*60)
        print("ğŸ¯ å¼€å§‹ä¿å­˜ç»“æœå’Œæå–åª’ä½“æ–‡ä»¶...")
        print("="*60)
        
        # ä¿å­˜å®Œæ•´JSONç»“æœ
        complete_file = self.raw_data_dir / "complete_crawl_result.json"
        with open(complete_file, "w", encoding="utf-8") as f:
            json.dump(status, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ å®Œæ•´çˆ¬å–ç»“æœå·²ä¿å­˜: {complete_file}")
        
        # æå–å¹¶ä¿å­˜é¡µé¢å†…å®¹
        if 'data' in status:
            pages = status['data']
            print(f"ğŸ“„ å¼€å§‹å¤„ç† {len(pages)} ä¸ªé¡µé¢...")
            
            # åˆ†ç±»ä¿å­˜å†…å®¹
            self.extract_and_categorize_content(pages)
            
            # æå–å¹¶ä¸‹è½½æ‰€æœ‰åª’ä½“æ–‡ä»¶
            self.extract_and_download_all_media(pages)
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            self.generate_final_report(pages)
    
    def extract_and_categorize_content(self, pages):
        """æå–å’Œåˆ†ç±»é¡µé¢å†…å®¹"""
        print("ğŸ“š å¼€å§‹åˆ†ç±»ä¿å­˜é¡µé¢å†…å®¹...")
        
        categories = {
            'lessons': [],
            'speaking': [],
            'listening': [],
            'grammar': [],
            'vocabulary': [],
            'pronunciation': [],
            'beginner': [],
            'intermediate': [],
            'advanced': [],
            'business': [],
            'travel': [],
            'toefl': [],
            'ielts': [],
            'interview': [],
            'practice': [],
            'other': []
        }
        
        for i, page in enumerate(pages, 1):
            if i % 100 == 0:
                print(f"  å¤„ç†è¿›åº¦: {i}/{len(pages)} é¡µé¢")
            
            url = page.get('metadata', {}).get('url', '')
            title = page.get('metadata', {}).get('title', 'Untitled')
            content = page.get('markdown', '')
            html_content = page.get('html', '')
            
            # åˆ†ç±»é¡µé¢
            category = 'other'
            for cat in categories.keys():
                if f"/{cat}/" in url.lower() or cat in url.lower():
                    category = cat
                    break
            
            # æå–åª’ä½“URL
            media_urls = self.extract_media_urls(content + html_content)
            
            page_info = {
                'url': url,
                'title': title,
                'markdown': content,
                'html': html_content,
                'word_count': len(content.split()) if content else 0,
                'char_count': len(content) if content else 0,
                'media_urls': media_urls,
                'media_count': {
                    'audio': len(media_urls['audio']),
                    'video': len(media_urls['video']),
                    'images': len(media_urls['images'])
                }
            }
            
            categories[category].append(page_info)
        
        # ä¿å­˜åˆ†ç±»ç»“æœ
        for category, pages_list in categories.items():
            if pages_list:
                category_file = self.content_dir / category / f"{category}_content.json"
                with open(category_file, "w", encoding="utf-8") as f:
                    json.dump(pages_list, f, indent=2, ensure_ascii=False)
                
                total_media = sum(page['media_count']['audio'] + page['media_count']['video'] + page['media_count']['images'] 
                                for page in pages_list)
                print(f"  ğŸ“‚ {category.title()}: {len(pages_list)} é¡µé¢, {total_media} ä¸ªåª’ä½“æ–‡ä»¶")
        
        print("âœ… å†…å®¹åˆ†ç±»å®Œæˆ")
    
    def extract_and_download_all_media(self, pages):
        """æå–å¹¶ä¸‹è½½æ‰€æœ‰åª’ä½“æ–‡ä»¶"""
        print("\nğŸµ å¼€å§‹æå–å’Œä¸‹è½½åª’ä½“æ–‡ä»¶...")
        
        all_media = {
            'audio': set(),
            'video': set(),
            'images': set()
        }
        
        media_info = {
            'audio': [],
            'video': [],
            'images': []
        }
        
        # æ”¶é›†æ‰€æœ‰åª’ä½“URL
        for i, page in enumerate(pages, 1):
            if i % 100 == 0:
                print(f"  æ‰«æè¿›åº¦: {i}/{len(pages)} é¡µé¢")
            
            url = page.get('metadata', {}).get('url', '')
            title = page.get('metadata', {}).get('title', 'Untitled')
            content = page.get('markdown', '') + page.get('html', '')
            
            media_urls = self.extract_media_urls(content)
            
            for media_type, urls in media_urls.items():
                for media_url in urls:
                    if media_url not in all_media[media_type]:
                        all_media[media_type].add(media_url)
                        media_info[media_type].append({
                            'url': media_url,
                            'source_page': url,
                            'source_title': title
                        })
        
        print(f"\nğŸ“Š åª’ä½“æ–‡ä»¶ç»Ÿè®¡:")
        print(f"  ğŸµ éŸ³é¢‘æ–‡ä»¶: {len(all_media['audio'])} ä¸ª")
        print(f"  ğŸ¬ è§†é¢‘æ–‡ä»¶: {len(all_media['video'])} ä¸ª")
        print(f"  ğŸ–¼ï¸  å›¾ç‰‡æ–‡ä»¶: {len(all_media['images'])} ä¸ª")
        print(f"  ğŸ“ æ€»è®¡: {sum(len(urls) for urls in all_media.values())} ä¸ªåª’ä½“æ–‡ä»¶")
        
        # ä¸‹è½½æ‰€æœ‰åª’ä½“æ–‡ä»¶
        downloaded_media = {
            'audio': [],
            'video': [],
            'images': []
        }
        
        for media_type, media_list in media_info.items():
            if not media_list:
                continue
                
            print(f"\nâ¬‡ï¸  å¼€å§‹ä¸‹è½½ {media_type} æ–‡ä»¶ ({len(media_list)} ä¸ª)...")
            
            for i, info in enumerate(media_list, 1):
                media_url = info['url']
                
                print(f"  [{i}/{len(media_list)}] ä¸‹è½½ {media_type}: {os.path.basename(media_url)}")
                
                result = self.download_media_file(media_url, media_type)
                if result:
                    result.update({
                        'source_page': info['source_page'],
                        'source_title': info['source_title']
                    })
                    downloaded_media[media_type].append(result)
                    self.media_stats[media_type]['downloaded'] += 1
                    print(f"    âœ… æˆåŠŸ: {result['filename']} ({result['file_size']} bytes)")
                else:
                    self.media_stats[media_type]['failed'] += 1
                    print(f"    âŒ å¤±è´¥: {media_url}")
                
                # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                time.sleep(0.5)
        
        # ä¿å­˜åª’ä½“ä¸‹è½½æŠ¥å‘Š
        media_report = {
            'timestamp': self.timestamp,
            'total_found': {k: len(v) for k, v in all_media.items()},
            'download_stats': self.media_stats,
            'downloaded_files': downloaded_media
        }
        
        media_report_file = self.reports_dir / "media_download_report.json"
        with open(media_report_file, "w", encoding="utf-8") as f:
            json.dump(media_report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ‰ åª’ä½“ä¸‹è½½å®Œæˆ!")
        for media_type, stats in self.media_stats.items():
            print(f"  {media_type.title()}: {stats['downloaded']}/{stats['found']} æˆåŠŸ, {stats['failed']} å¤±è´¥")
    
    def generate_final_report(self, pages):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_pages = len(pages)
        total_words = sum(len(page.get('markdown', '').split()) for page in pages)
        total_chars = sum(len(page.get('markdown', '')) for page in pages)
        
        # æŒ‰åŸŸåç»Ÿè®¡
        domain_stats = {}
        for page in pages:
            url = page.get('metadata', {}).get('url', '')
            domain = urlparse(url).netloc
            if domain not in domain_stats:
                domain_stats[domain] = 0
            domain_stats[domain] += 1
        
        # ç”ŸæˆæŠ¥å‘Š
        final_report = {
            'session_info': {
                'timestamp': self.timestamp,
                'session_directory': str(self.session_dir.absolute()),
                'config_file': self.config_file,
                'crawl_duration': str(datetime.now() - datetime.strptime(self.timestamp, "%Y%m%d_%H%M%S"))
            },
            'content_stats': {
                'total_pages': total_pages,
                'total_words': total_words,
                'total_characters': total_chars,
                'avg_words_per_page': total_words // total_pages if total_pages > 0 else 0,
                'domain_distribution': domain_stats
            },
            'media_stats': {
                'found': {k: v['found'] for k, v in self.media_stats.items()},
                'downloaded': {k: v['downloaded'] for k, v in self.media_stats.items()},
                'failed': {k: v['failed'] for k, v in self.media_stats.items()},
                'success_rate': {
                    k: round(v['downloaded'] / v['found'] * 100, 2) if v['found'] > 0 else 0 
                    for k, v in self.media_stats.items()
                }
            },
            'directory_structure': {
                'audio_files': len(list(self.audio_dir.glob('*'))),
                'video_files': len(list(self.video_dir.glob('*'))),
                'image_files': len(list(self.images_dir.glob('*'))),
                'content_categories': len(list(self.content_dir.glob('*')))
            }
        }
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        final_report_file = self.reports_dir / "final_report.json"
        with open(final_report_file, "w", encoding="utf-8") as f:
            json.dump(final_report, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆå¯è¯»æ€§æŠ¥å‘Š
        readable_report = self.reports_dir / "summary_report.txt"
        with open(readable_report, "w", encoding="utf-8") as f:
            f.write(f"TalkEnglish.com å…¨åª’ä½“çˆ¬å–æŠ¥å‘Š\n")
            f.write(f"=" * 50 + "\n\n")
            f.write(f"ä¼šè¯ä¿¡æ¯:\n")
            f.write(f"  æ—¶é—´æˆ³: {self.timestamp}\n")
            f.write(f"  ç›®å½•: {self.session_dir.absolute()}\n\n")
            f.write(f"å†…å®¹ç»Ÿè®¡:\n")
            f.write(f"  æ€»é¡µé¢æ•°: {total_pages:,}\n")
            f.write(f"  æ€»è¯æ•°: {total_words:,}\n")
            f.write(f"  æ€»å­—ç¬¦æ•°: {total_chars:,}\n")
            f.write(f"  å¹³å‡æ¯é¡µè¯æ•°: {total_words // total_pages if total_pages > 0 else 0}\n\n")
            f.write(f"åª’ä½“æ–‡ä»¶ç»Ÿè®¡:\n")
            for media_type, stats in self.media_stats.items():
                f.write(f"  {media_type.title()}:\n")
                f.write(f"    å‘ç°: {stats['found']} ä¸ª\n")
                f.write(f"    ä¸‹è½½æˆåŠŸ: {stats['downloaded']} ä¸ª\n")
                f.write(f"    ä¸‹è½½å¤±è´¥: {stats['failed']} ä¸ª\n")
                f.write(f"    æˆåŠŸç‡: {stats['downloaded'] / stats['found'] * 100 if stats['found'] > 0 else 0:.1f}%\n\n")
            f.write(f"ç›®å½•ç»“æ„:\n")
            f.write(f"  éŸ³é¢‘æ–‡ä»¶: {len(list(self.audio_dir.glob('*')))} ä¸ª\n")
            f.write(f"  è§†é¢‘æ–‡ä»¶: {len(list(self.video_dir.glob('*')))} ä¸ª\n")
            f.write(f"  å›¾ç‰‡æ–‡ä»¶: {len(list(self.images_dir.glob('*')))} ä¸ª\n")
            f.write(f"  å†…å®¹åˆ†ç±»: {len(list(self.content_dir.glob('*')))} ä¸ª\n")
        
        print(f"ğŸ“‹ æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ:")
        print(f"  - JSONæŠ¥å‘Š: {final_report_file}")
        print(f"  - æ–‡æœ¬æ‘˜è¦: {readable_report}")
        print(f"\nğŸ‰ å…¨åª’ä½“çˆ¬å–å®Œæˆ!")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {self.session_dir.absolute()}")

def main():
    print("ğŸŒ TalkEnglish.com å…¨åª’ä½“çˆ¬å–å™¨")
    print("=" * 60)
    print("ğŸ¯ åŠŸèƒ½: çˆ¬å–ç½‘ç«™æ‰€æœ‰å†…å®¹ï¼ŒåŒ…æ‹¬éŸ³é¢‘ã€è§†é¢‘ã€å›¾ç‰‡")
    print("ğŸ“ ç»“æœ: æŒ‰æ—¥æœŸç»„ç»‡åˆ° results/talkenglish_YYYYMMDD_HHMMSS/ ç›®å½•")
    print("=" * 60)
    
    crawler = TalkEnglishFullCrawler()
    
    print(f"\nğŸ“… å½“å‰ä¼šè¯: {crawler.timestamp}")
    print(f"ğŸ“ ç»“æœç›®å½•: {crawler.session_dir.absolute()}")
    
    # å¯åŠ¨çˆ¬å–
    crawl_id = crawler.start_crawl()
    if crawl_id:
        # ç›‘æ§è¿›åº¦
        crawler.monitor_crawl(crawl_id, check_interval=90)
    
    print(f"\nğŸ“‚ ç›®å½•ç»“æ„:")
    print(f"  {crawler.session_dir.name}/")
    print(f"  â”œâ”€â”€ audio/           # éŸ³é¢‘æ–‡ä»¶ (MP3, WAVç­‰)")
    print(f"  â”œâ”€â”€ video/           # è§†é¢‘æ–‡ä»¶ (MP4, WebMç­‰)")
    print(f"  â”œâ”€â”€ images/          # å›¾ç‰‡æ–‡ä»¶ (JPG, PNGç­‰)")
    print(f"  â”œâ”€â”€ content/         # æŒ‰ç±»åˆ«åˆ†ç±»çš„å†…å®¹")
    print(f"  â”œâ”€â”€ reports/         # çˆ¬å–æŠ¥å‘Šå’Œç»Ÿè®¡")
    print(f"  â””â”€â”€ raw_data/        # åŸå§‹çˆ¬å–æ•°æ®")

if __name__ == "__main__":
    main()