import requests
import json
import time
from datetime import datetime

class OptimizedTalkEnglishCrawler:
    def __init__(self, base_url="http://localhost:3002", config_file="talkenglish_optimized.json"):
        self.base_url = base_url
        self.config_file = config_file
        self.headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        }
    
    def load_config(self):
        """åŠ è½½ä¼˜åŒ–é…ç½®"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print(f"âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶: {self.config_file}")
            return config
        except FileNotFoundError:
            print(f"âŒ é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {self.config_file}")
            return None
        except json.JSONDecodeError as e:
            print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            return None
    
    def start_crawl(self):
        """ä½¿ç”¨ä¼˜åŒ–é…ç½®å¯åŠ¨çˆ¬å–"""
        config = self.load_config()
        if not config:
            return None
        
        print("ğŸš€ ä½¿ç”¨ä¼˜åŒ–é…ç½®å¯åŠ¨ TalkEnglish.com å…¨ç«™çˆ¬å–...")
        print(f"ğŸ“Š é…ç½®è¯¦æƒ…:")
        print(f"  - é¡µé¢é™åˆ¶: {config.get('limit', 'N/A')}")
        print(f"  - æœ€å¤§æ·±åº¦: {config.get('maxDepth', 'N/A')}")
        print(f"  - ç­‰å¾…æ—¶é—´: {config.get('scrapeOptions', {}).get('waitFor', 'N/A')}ms")
        print(f"  - è¶…æ—¶æ—¶é—´: {config.get('scrapeOptions', {}).get('timeout', 'N/A')}ms")
        print(f"  - åŒ…å«è·¯å¾„: {len(config.get('includePaths', []))} ä¸ª")
        print(f"  - æ’é™¤è·¯å¾„: {len(config.get('excludePaths', []))} ä¸ª")
        
        response = requests.post(
            f"{self.base_url}/v1/crawl",
            headers=self.headers,
            json=config
        )
        
        if response.status_code == 200:
            result = response.json()
            crawl_id = result.get('id')
            print(f"âœ… çˆ¬å–ä»»åŠ¡å·²å¯åŠ¨ï¼ŒID: {crawl_id}")
            
            # ä¿å­˜åˆå§‹å“åº”
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            with open(f"talkenglish_optimized_crawl_{timestamp}.json", "w", encoding="utf-8") as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            return crawl_id
        else:
            print(f"âŒ å¯åŠ¨çˆ¬å–å¤±è´¥: {response.status_code}")
            print(response.text)
            return None
    
    def check_status(self, crawl_id):
        """æ£€æŸ¥çˆ¬å–çŠ¶æ€"""
        response = requests.get(
            f"{self.base_url}/v1/crawl/{crawl_id}",
            headers=self.headers
        )
        
        if response.status_code == 200:
            return response.json()
        else:
            print(f"âŒ æ£€æŸ¥çŠ¶æ€å¤±è´¥: {response.status_code}")
            return None
    
    def monitor_crawl(self, crawl_id, check_interval=60):
        """ç›‘æ§çˆ¬å–è¿›åº¦ï¼ˆä¼˜åŒ–é…ç½®å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´ï¼‰"""
        print(f"ğŸ“Š å¼€å§‹ç›‘æ§çˆ¬å–è¿›åº¦ (æ¯{check_interval}ç§’æ£€æŸ¥ä¸€æ¬¡)...")
        print("âš ï¸  ç”±äºä½¿ç”¨äº†ä¼˜åŒ–é…ç½®ï¼Œçˆ¬å–å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        start_time = datetime.now()
        
        while True:
            status = self.check_status(crawl_id)
            if not status:
                break
            
            current_time = datetime.now().strftime("%H:%M:%S")
            elapsed = datetime.now() - start_time
            
            if status.get('status') == 'completed':
                print(f"ğŸ‰ [{current_time}] çˆ¬å–å®Œæˆ! æ€»è€—æ—¶: {elapsed}")
                print(f"ğŸ“„ æ€»é¡µé¢æ•°: {len(status.get('data', []))}")
                
                # ä¿å­˜å®Œæ•´ç»“æœ
                self.save_results(crawl_id, status)
                break
            
            elif status.get('status') == 'scraping':
                completed = len(status.get('data', []))
                print(f"â³ [{current_time}] çˆ¬å–ä¸­... å·²å®Œæˆ: {completed} é¡µé¢ (è€—æ—¶: {elapsed})")
            
            elif status.get('status') == 'failed':
                print(f"âŒ [{current_time}] çˆ¬å–å¤±è´¥ (è€—æ—¶: {elapsed})")
                print(f"é”™è¯¯ä¿¡æ¯: {status.get('error', 'Unknown error')}")
                break
            
            time.sleep(check_interval)
    
    def save_results(self, crawl_id, status):
        """ä¿å­˜çˆ¬å–ç»“æœå¹¶æŒ‰ç±»åˆ«åˆ†ç±»"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å®Œæ•´JSONç»“æœ
        filename = f"talkenglish_optimized_complete_{timestamp}.json"
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(status, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        
        # æå–å¹¶ä¿å­˜é¡µé¢å†…å®¹
        if 'data' in status:
            self.extract_and_categorize_content(status['data'], timestamp)
    
    def extract_and_categorize_content(self, pages, timestamp):
        """æå–å’Œåˆ†ç±»é¡µé¢å†…å®¹ï¼ˆåŸºäºä¼˜åŒ–é…ç½®çš„è·¯å¾„ï¼‰"""
        categories = {
            'lessons': [],
            'speaking': [],
            'listening': [],
            'grammar': [],
            'vocabulary': [],
            'pronunciation': [],
            'beginner': [],
            'intermediate': [],
            'advanced': [],
            'business': [],
            'travel': [],
            'toefl': [],
            'ielts': [],
            'interview': [],
            'other': []
        }
        
        for page in pages:
            url = page.get('metadata', {}).get('url', '')
            title = page.get('metadata', {}).get('title', 'Untitled')
            content = page.get('markdown', '')
            
            # åˆ†ç±»é¡µé¢ï¼ˆåŸºäºURLè·¯å¾„ï¼‰
            category = 'other'
            for cat in categories.keys():
                if f"/{cat}/" in url.lower():
                    category = cat
                    break
            
            page_info = {
                'url': url,
                'title': title,
                'content': content,
                'word_count': len(content.split()) if content else 0,
                'char_count': len(content) if content else 0
            }
            
            categories[category].append(page_info)
        
        # ä¿å­˜åˆ†ç±»ç»“æœ
        for category, pages_list in categories.items():
            if pages_list:
                filename = f"talkenglish_optimized_{category}_{timestamp}.json"
                with open(filename, "w", encoding="utf-8") as f:
                    json.dump(pages_list, f, indent=2, ensure_ascii=False)
                print(f"ğŸ“š {category.title()} å†…å®¹å·²ä¿å­˜åˆ°: {filename} ({len(pages_list)} é¡µé¢)")
        
        # ç”Ÿæˆè¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š
        self.generate_detailed_report(categories, timestamp)
    
    def generate_detailed_report(self, categories, timestamp):
        """ç”Ÿæˆè¯¦ç»†çš„çˆ¬å–æŠ¥å‘Š"""
        total_pages = sum(len(pages) for pages in categories.values())
        total_words = sum(sum(page['word_count'] for page in pages) for pages in categories.values())
        
        report = {
            'timestamp': timestamp,
            'config_file': self.config_file,
            'total_pages': total_pages,
            'total_words': total_words,
            'categories': {}
        }
        
        for category, pages in categories.items():
            if pages:
                category_words = sum(page['word_count'] for page in pages)
                category_chars = sum(page['char_count'] for page in pages)
                report['categories'][category] = {
                    'page_count': len(pages),
                    'total_words': category_words,
                    'total_chars': category_chars,
                    'avg_words_per_page': category_words // len(pages) if pages else 0,
                    'percentage_of_total': round((len(pages) / total_pages) * 100, 2) if total_pages > 0 else 0
                }
        
        filename = f"talkenglish_optimized_report_{timestamp}.json"
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“Š è¯¦ç»†çˆ¬å–æŠ¥å‘Š:")
        print(f"é…ç½®æ–‡ä»¶: {self.config_file}")
        print(f"æ€»é¡µé¢æ•°: {report['total_pages']}")
        print(f"æ€»è¯æ•°: {report['total_words']:,}")
        print(f"\nå„ç±»åˆ«ç»Ÿè®¡:")
        for category, stats in report['categories'].items():
            print(f"  {category.title()}: {stats['page_count']} é¡µé¢ ({stats['percentage_of_total']}%), {stats['total_words']:,} è¯")
        print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")

def main():
    crawler = OptimizedTalkEnglishCrawler()
    
    # å¯åŠ¨çˆ¬å–
    crawl_id = crawler.start_crawl()
    if crawl_id:
        # ç›‘æ§è¿›åº¦ï¼ˆä½¿ç”¨æ›´é•¿çš„æ£€æŸ¥é—´éš”ï¼Œå› ä¸ºä¼˜åŒ–é…ç½®å¯èƒ½éœ€è¦æ›´å¤šæ—¶é—´ï¼‰
        crawler.monitor_crawl(crawl_id, check_interval=60)

if __name__ == "__main__":
    main()