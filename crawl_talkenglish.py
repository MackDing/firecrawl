import requests
import json
import time
import os
from datetime import datetime

class TalkEnglishCrawler:
    def __init__(self, base_url="http://localhost:3002"):
        self.base_url = base_url
        self.headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        }
    
    def start_crawl(self):
        """å¯åŠ¨çˆ¬å–ä»»åŠ¡"""
        crawl_config = {
            "url": "https://www.talkenglish.com/",
            "limit": 2000,  # å¢åŠ é¡µé¢é™åˆ¶
            "scrapeOptions": {
                "formats": ["markdown", "html"],
                "onlyMainContent": True,
                "waitFor": 3000,
                "timeout": 45000,
                "headers": {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                }
            },
            "maxDepth": 6,
            "allowBackwardLinks": False,
            "allowExternalLinks": False,
            "includePaths": [
                "/lessons/*",
                "/speaking/*", 
                "/listening/*",
                "/grammar/*",
                "/vocabulary/*",
                "/pronunciation/*",
                "/beginner/*",
                "/intermediate/*",
                "/advanced/*",
                "/business/*",
                "/travel/*"
            ],
            "excludePaths": [
                "/admin/*",
                "/login/*",
                "/register/*",
                "*.pdf",
                "*.mp3",
                "*.wav"
            ]
        }
        
        print("ğŸš€ å¯åŠ¨ TalkEnglish.com å…¨ç«™çˆ¬å–...")
        response = requests.post(
            f"{self.base_url}/v1/crawl",
            headers=self.headers,
            json=crawl_config
        )
        
        if response.status_code == 200:
            result = response.json()
            crawl_id = result.get('id')
            print(f"âœ… çˆ¬å–ä»»åŠ¡å·²å¯åŠ¨ï¼ŒID: {crawl_id}")
            
            # ä¿å­˜åˆå§‹å“åº”
            with open(f"talkenglish_crawl_{crawl_id}.json", "w", encoding="utf-8") as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            return crawl_id
        else:
            print(f"âŒ å¯åŠ¨çˆ¬å–å¤±è´¥: {response.status_code}")
            print(response.text)
            return None
    
    def check_status(self, crawl_id):
        """æ£€æŸ¥çˆ¬å–çŠ¶æ€"""
        response = requests.get(
            f"{self.base_url}/v1/crawl/{crawl_id}",
            headers=self.headers
        )
        
        if response.status_code == 200:
            return response.json()
        else:
            print(f"âŒ æ£€æŸ¥çŠ¶æ€å¤±è´¥: {response.status_code}")
            return None
    
    def monitor_crawl(self, crawl_id, check_interval=30):
        """ç›‘æ§çˆ¬å–è¿›åº¦"""
        print(f"ğŸ“Š å¼€å§‹ç›‘æ§çˆ¬å–è¿›åº¦ (æ¯{check_interval}ç§’æ£€æŸ¥ä¸€æ¬¡)...")
        
        while True:
            status = self.check_status(crawl_id)
            if not status:
                break
            
            current_time = datetime.now().strftime("%H:%M:%S")
            
            if status.get('status') == 'completed':
                print(f"ğŸ‰ [{current_time}] çˆ¬å–å®Œæˆ!")
                print(f"ğŸ“„ æ€»é¡µé¢æ•°: {len(status.get('data', []))}")
                
                # ä¿å­˜å®Œæ•´ç»“æœ
                self.save_results(crawl_id, status)
                break
            
            elif status.get('status') == 'scraping':
                completed = len(status.get('data', []))
                print(f"â³ [{current_time}] çˆ¬å–ä¸­... å·²å®Œæˆ: {completed} é¡µé¢")
            
            elif status.get('status') == 'failed':
                print(f"âŒ [{current_time}] çˆ¬å–å¤±è´¥")
                print(f"é”™è¯¯ä¿¡æ¯: {status.get('error', 'Unknown error')}")
                break
            
            time.sleep(check_interval)
    
    def save_results(self, crawl_id, status):
        """ä¿å­˜çˆ¬å–ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å®Œæ•´JSONç»“æœ
        filename = f"talkenglish_complete_{timestamp}.json"
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(status, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        
        # æå–å¹¶ä¿å­˜é¡µé¢å†…å®¹
        if 'data' in status:
            self.extract_content(status['data'], timestamp)
    
    def extract_content(self, pages, timestamp):
        """æå–å’Œåˆ†ç±»é¡µé¢å†…å®¹"""
        categories = {
            'lessons': [],
            'speaking': [],
            'listening': [],
            'grammar': [],
            'vocabulary': [],
            'pronunciation': [],
            'other': []
        }
        
        for page in pages:
            url = page.get('metadata', {}).get('url', '')
            title = page.get('metadata', {}).get('title', 'Untitled')
            content = page.get('markdown', '')
            
            # åˆ†ç±»é¡µé¢
            category = 'other'
            for cat in categories.keys():
                if cat in url.lower():
                    category = cat
                    break
            
            categories[category].append({
                'url': url,
                'title': title,
                'content': content,
                'word_count': len(content.split()) if content else 0
            })
        
        # ä¿å­˜åˆ†ç±»ç»“æœ
        for category, pages in categories.items():
            if pages:
                filename = f"talkenglish_{category}_{timestamp}.json"
                with open(filename, "w", encoding="utf-8") as f:
                    json.dump(pages, f, indent=2, ensure_ascii=False)
                print(f"ğŸ“š {category.title()} å†…å®¹å·²ä¿å­˜åˆ°: {filename} ({len(pages)} é¡µé¢)")
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.generate_report(categories, timestamp)
    
    def generate_report(self, categories, timestamp):
        """ç”Ÿæˆçˆ¬å–æŠ¥å‘Š"""
        report = {
            'timestamp': timestamp,
            'total_pages': sum(len(pages) for pages in categories.values()),
            'categories': {}
        }
        
        for category, pages in categories.items():
            if pages:
                total_words = sum(page['word_count'] for page in pages)
                report['categories'][category] = {
                    'page_count': len(pages),
                    'total_words': total_words,
                    'avg_words_per_page': total_words // len(pages) if pages else 0
                }
        
        filename = f"talkenglish_report_{timestamp}.json"
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“Š çˆ¬å–æŠ¥å‘Š:")
        print(f"æ€»é¡µé¢æ•°: {report['total_pages']}")
        for category, stats in report['categories'].items():
            print(f"  {category.title()}: {stats['page_count']} é¡µé¢, {stats['total_words']} è¯")
        print(f"è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")

def main():
    crawler = TalkEnglishCrawler()
    
    # å¯åŠ¨çˆ¬å–
    crawl_id = crawler.start_crawl()
    if crawl_id:
        # ç›‘æ§è¿›åº¦
        crawler.monitor_crawl(crawl_id)

if __name__ == "__main__":
    main()